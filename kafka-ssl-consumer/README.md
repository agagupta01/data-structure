# Kafka SSL Consumer

This project provides a simple executable example of consuming messages from a Kafka topic.
The configuration for the project provides all the properties required to connect to the
Quote API's Kafka cluster which has been configured to use two-way-SSL.

The `main()` method of the KafkaTopicConsumer class loads the properties from a file and then consumes from the chosen topic.
With the provided configuration, offsets are never committed, therefore this application can be run multiple
times and the output will always consume all messages from the earliest offset on the topic.

## Usage instructions

```shell
$ java -Dconsumer.properties=<path_to_properties_file> \
    -jar kafka-ssl-consumer-1.0-all.jar <topic_name> <number_of_messages>
```

The properties file will need to define the bootstrap servers along with the location to the truststore and keystore to
be used by the application and their corresponding passwords. The easiest way to do this is to place all four files that
are required in the same directory and just use file names throughout the configuration and the above command.
The required files are:

* The fat jar built by this module (*kafka-ssl-consumer-1.0-all.jar*).
* The client truststore (given to clients by the Quote API, found in S3: tesco-price-service-kafka/certs/ppe/broker/kafka.client.truststore.jks).
* The client certificate keystore (generated by clients using the instructions in the `../client-certificate-generation` directory, current client certs found in our S3 bucket:  tesco-price-service-kafka/certs/).
* The consumer.properties file (example can be found in this project) with the correct credentials:
  * ssl.truststore.password - the SSL truststore file password of the broker (provided by the Quote API).
  * ssl.keystore.password - the keystore file password of the consumer (created by clients).
  * ssl.key.password - the key password which is inside the keystore of the consumer (created by clients).

Once built, this project could be placed on an instance in a VPC/VNet or deployed in a container-based environment to verify
connectivity to our clusters prior to production deployment. The log level can be changed in the `./src/main/resources/log4j.properties`
file which is currently defaulted to be TRACE level debugging which should provide as much information as is needed to debug
connectivity issues.

*Note: While using this consumer to connect to live Kafka cluster, please use producer keystore and password, the consumer keystore will not work. This is due to slightly incorrect/confusing configuration of the ACLs for the topic within price_aws repo. The CN within the ACLs for the live topics are confiured as "Price Service Producer" instead of "Price Service Consumer". This note is included here because people intending to consume from Live kafka cluster will go through this page while configuring the consumer.* 
